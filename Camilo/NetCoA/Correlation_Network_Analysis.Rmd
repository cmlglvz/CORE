---
title: "Correlation Network Analysis"
author: "Lucas Gallart"
edited: "Camilo Gálvez A."
output: html_document
---

#Libraries
```{r libraries, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(psych)
library(igraph)
library(ggpubr)
```

#Relative abundance
Calcula la abundancia total por muestra/sitio y divide la abundancia de cada ASV entre esa
```{r rel abun, echo=TRUE,message=FALSE,warning=FALSE,include=FALSE}
rel.abun <- function(data){
  total <- rowSums(data)
  rltv <- data.frame()
  for (x in 1:length(data[,1])) {
    rltv <- rbind(rltv, data[x,]/total[x])
    rltv[is.na(rltv)] <- 0
  }
  final <- data.frame("colMeans.(t)." = colMeans(rltv),"From" = colnames(data))
}
```

#Correlation and structuring data
Para facilitar el trabajo del código llamaremos a las tablas de abundancia *"t"* y les daremos un número correlativo correspondiente: Chañaral, Flamenco, Huasco, Punta de Choros, Quintero, Las Cruces.
En caso de tener problemas: 1) Inica R como administrador; 2) Crea las carpetas antes de correr el scritp; o 3) Indica minuciosamente el recorrido del directorio correspondiente
```{r struc, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
setwd("D:/Documents/GitHub/CORE/Camilo/NetCoA/")
node_t <- list()
for (y in 1:6) {
  #Llamamos a los datos, con str_glue logramos que llame a las 6 distintas
  t <- as.data.frame(read.table(str_glue("t{y}.csv"), sep = ";", dec = ".", h = TRUE, row.names = 1))
  #Llamamos la función de abundancias relativas
  abundance <- rel.abun(data = t)
  #Análisis de correlación de Spearman ajustando el valor de p con false discovery rate
  tcor <- psych::corr.test(t, method = "spearman", adjust = "fdr", alpha = .05, ci = FALSE)
  #Guardamos el resultado con valores de r (una tabla)
  write.table(tcor$r, str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{y}/tr{y}.csv"), sep = "\t", dec = ".")
  #Guardamos el resultado con valores de p (una tabla)
  write.table(tcor$p, str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{y}/tr{y}.csv"), sep = "\t", dec = ".")
  #Preparamos unos objetos para el loop
  names <- NULL
  datar <- NULL
  datap <- NULL
  for (i in 1:dim(tcor$r)[1]) {
    #Vector con los nombres correspondientes a cada correlación (ASV0001-ASV0002), con el loop recorre la matriz de correlación registrando que cada ASV interactue con el resto
    names <- c(names, paste(rownames(tcor$p)[i], colnames(tcor$p), sep = " - "))
    #Guardamos todos los valores de r de la matriz de correlación siguiendo el mismo orden que para names
    datar <- c(datar, as.matrix(tcor$r)[i,])
    #Guardamos todos los valores de p de la matriz de correlación siguiendo el mismo orden
    datap <- c(datap, as.matrix(tcor$p)[i,])
  }
  #Unimos todo en una tabla (como estaban en el mismo orden no hay problema) y filtramos por valores de p y r
  unionfr <- data.frame(ASV = names, rValue = datar, pValue = datap) %>% 
    dplyr::filter(rValue >= 0.6 | rValue <= -0.6, pValue != 0 & pValue <= 0.05)
  tcytos <- data.frame(From = stringr::str_sub(unionfr[,1], start = 1, end = 7), 
                       to = stringr::str_sub(unionfr[,1], start = 11, end = 17), 
                       rValue = unionfr[,2], pValue = unionfr[,3]) %>% 
    dplyr::filter(!From == to)
  #Creamos un objeto en el que vamos a ir guardando las características de cada nodo, partiendo por su abundancia relativa.
  #Solo nos quedaremos con aquellos nodos que hayan representado al menos una interacción importante (que haya pasado el filtro)
  node_t[[y]] <- abundance %>% dplyr::filter(From %in% c(tcytos$From, tcytos$to)) %>% rename("Node" = "From")
  write.table(node_t[[y]],str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{y}/node_mat_t{y}.csv",sep = "\t",dec = ".",row.names = FALSE))
  write.table(tcytos,str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{y}/adj_mat_t{y}.csv"),sep = "\t",dec = ".",row.names = FALSE)
}

#view(tcytos)
#view(node_t[[1]])
```

#Node type identification
```{r ntype, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
path_u <- cbind(rep("HCS_Unique_", 6), c("001", "002", "003", "004", "005", "006"), rep("_PPE_ASV_Abundance_Network_Analysis.csv", 6))
shared <- read.table("HCS_Shared_PPE_ASV_Abundance_Network_Analysis.csv", sep = ";", h = FALSE)

node_t_class <- list()
ulist <- list()
for (i in 1:dim(path_u)[1]) {
  ulist[[i]] <- read.table(paste0(path_u[i,1], path_u[i,2], path_u[i,3]), sep = ";", h = FALSE)
  node_t_class[[i]] <- node_t[[i]] %>% 
    dplyr::mutate("Node_Category" = case_when(Node %in%  t(ulist[[i]])[,1]~"Unique", Node %in% t(shared[1,-1])~"Shared", TRUE~"Other"))
  write.table(node_t_class[[i]],str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{i}/node_mat_t{i}.csv"), sep = "\t", dec = ".")
}

par(mfrow = c(2,3))
ASVs <- data.frame("Other" = NA, "Shared" = NA, "Unique" = NA)
for (i in 1:6) {
  plot(table(node_t_class[[i]]$Node_Category), main = path_u[i,2], ylab = "ASVs")
  ASVs[i,] <- table(node_t_class[[i]]$Node_Category)
}
ASVs2 <- ASVs %>% dplyr::mutate(Site = path_u[,2])
write.table(ASVs2, "D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/other_results/ASVs_class.csv", row.names = FALSE)
```

#Igraph object
```{r igraph, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
gc()
edgelist <- list()
nodelist <- list()
graphlist <- list()
for (x in 1:6) {
  nodelist[[x]] <- read.table(str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{x}/node_mat_t{x}.csv"), sep = "\t", dec = ".", header = TRUE) %>% relocate(Node)
  data <- read.table(str_glue("D:/Documents/GitHub/CORE/Camilo/NetCoA/net_creation/t{x}/adj_mat_t{x}.csv"), sep = "\t", dec = ".", header = TRUE)
  #nodelist[[x]] <- node_t_class[[x]]
  edgelist[[x]] <- data %>% select(From, to)
  #nodelist[[x]] <- data %>% pivot_longer(c(From, to)) %>% distinct(value, .keep_all = TRUE) %>% select(value, colMeans..t..,From_type, To_type)
  graphlist[[x]] <- igraph::simplify(graph_from_data_frame(edgelist[[x]], directed = FALSE, vertices = as.data.frame(nodelist[[x]])))
}
```

#Fast analysis
Evaluar assortativity para Shared vs other
```{r assort, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
#assortativity <- NULL
cc <- NULL
avpath <- NULL
ctance <- NULL
mdeg <- NULL
modul <- NULL
clusters <- NULL
diam <- NULL
#results <- matrix(,12,1000)
size <- NULL
order <- NULL
for (x in 1:6) {
  V(graphlist[[x]])$degree <- degree(graphlist[[x]], mode = c("All"))
  V(graphlist[[x]])$eigen <- evcent(graphlist[[x]])$vector
  V(graphlist[[x]])$betweenness <- betweenness(graphlist[[x]], directed = FALSE)
  values <- as.factor(V(graphlist[[x]])$Tipo)
  #assortativity[x] <- assortativity_nominal(graphlist[[x]], types = values)
  size[x] <- gsize(graphlist[[x]])
  order[x] <- gorder(graphlist[[x]])
  cc[x] <- transitivity(graphlist[[x]], type = "average")
  avpath[x] <- mean_distance(graphlist[[x]], directed = TRUE, unconnected = TRUE)
  ctance[x] <-edge_density(graphlist[[x]], loops = FALSE)
  mdeg[x] <- mean(degree(graphlist[[x]]))
  modul[x] <-modularity(fastgreedy.community(graphlist[[x]], merges = FALSE, modularity = TRUE))
  clusters[x] <- length(fastgreedy.community(graphlist[[x]], merges = FALSE, modularity = TRUE))
  diam[x] <- diameter(graphlist[[x]], directed = FALSE, unconnected = TRUE)
  
  #for (y in 1:1000) {
  # results[x,y] <- assortativity_nominal(graphlist[[x]], sample(values))
  #}
}

path <- c("001", "002", "003", "004", "005", "006")

nets_stats <- data.frame("Connectance" = ctance, 
                         "Average_Degree" = mdeg, 
                         "Diameter" = diam, 
                         "Average_path_length" = avpath, 
                         "Cluster_coefficientt" = cc, 
                         "Modularity" = modul, 
                         "Number_of_clusters" = clusters, 
                         "Nodes" = order, 
                         "Edges" = size, 
                         "Net" = path) %>% relocate(Net)

barplot(nets_stats$Connectance~nets_stats$Net)
barplot(nets_stats$Nodes~nets_stats$Net)
barplot(nets_stats$Average_Degree~nets_stats$Net)
barplot(nets_stats$Average_path_length~nets_stats$Net)
barplot(nets_stats$Modularity~nets_stats$Net)
```

#Net plots
```{r nplots, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
sites <- c("Chañaral", "Flamenco", "Huasco", "Punta Choros", "Quintero", "Las Cruces")
pal <- c("#9CAFB7", "#2B2D42", "#EF233C")
par(mfrow = c(2,3))
for (x in 1:6) {
  set.seed(1000)
  plot(graphlist[[x]], edge.color = "grey", vertex.label = NA, 
       vertex.color = case_when(vertex_attr(graphlist[[x]], "Node_Category") == "Other"~pal[2], 
                                vertex_attr(graphlist[[x]], "Node_Category") == "Shared"~pal[3], 
                                vertex_attr(graphlist[[x]], "Node_Category") == "Unique"~pal[1]), 
       vertex_size = log(V(graphlist[[x]])$degree+2), edge.width = 0.1/10000, layout = layout.fruchterman.reingold, 
                         main = sites[x], sub = str_glue("N:{gorder(graphlist[[x]])}  E:{gsize(graphlist[[x]])}"))
}

par(new = FALSE)
```

#Degree plots
```{r dplots, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
nlist <- list(); deg <- list(); top_degs <- list(); plots <- list(); nod <- list(); deg_pre <- list()
for (i in 1:6) {
  nlist[[i]] <- data.frame("Node" = vertex_attr(graphlist[[i]], "name"), 
                           "Category" = vertex_attr(graphlist[[i]], "Node_Category"), 
                           "Degree" = vertex_attr(graphlist[[i]], "degree", ),
                           "Net" = path[i],
                           "Size" = gorder(graphlist[[i]]))
  
 deg_pre[[i]] <- nlist[[i]] %>% filter(Degree <= quantile(Degree, probs = 0.2))
 deg[[i]] <- as.data.frame(table(deg_pre[[i]]$Category)) %>% 
   mutate("Net" = path[i], "Nodos_upperquantile" = dim(deg_pre[[i]])[1],
           Proportion = Freq/Nodos_upperquantile, Total = Proportion)
 top_degs[[i]] <- as.data.frame(table(deg[[i]]$Category))
 plots[[i]] <- ggplot(top_degs[[i]], aes(y = Freq, x = Var1, fill = Var1)) + geom_col() + theme_bw()
 nod[[i]] <- as.data.frame(table(nlist[[i]]$Category)) %>% 
   dplyr::mutate("Net" = path[i], "Size" = gorder(graphlist[[i]]), Proportion = Freq/Size, Total = Proportion)
}
degs <- deg
degs[[4]] <- NULL
degs <- degs %>% bind_rows()
deg2 <- deg %>% bind_rows()
ggplot(deg, aes(y = Total, x = Net, fill = factor(Var1, levels = c("Other", "Unique", "Shared")))) + 
  geom_col() + 
  scale_fill_manual(name = "ASV Category", values = c("#9CAFB7", "#2B2D42", "#EF233C"))
ggplot(deg2, aes(y = Total, x = Net, fill = factor(Var1, levels = c("Other", "Unique", "Shared")))) + 
  geom_col() + 
  scale_fill_manual(name = "ASV Category", values = c("#9CAFB7", "#2B2D42", "#EF233C"))
nods <- nod
nods[[4]] <- NULL
nods <- nod %>% bind_rows()
nod2 <- nod %>% bind_rows()
ggplot(nods, aes(y = Total, x = Net, fill = factor(Var1, levels = c("Other", "Unique", "Shared")))) + 
  geom_col() + 
  scale_fill_manual(name = "ASV Category", values = c("#9CAFB7", "#2B2D42", "#EF233C"))
ggplot(nod2, aes(y = Total, x = Net, fill = factor(Var1, levels = c("Other", "Unique", "Shared")))) + 
  geom_col() + 
  scale_fill_manual(name = "ASV Category", values = c("#9CAFB7", "#2B2D42", "#EF233C"))

ggplot(nods, aes(y = Freq, x = Net, fill = Freq)) + geom_col()
ggarrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]])
```

#Clustering
```{r clustering, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
clusts <- NULL
mods <- NULL
for (y in 1:6) {
  fc <- fastgreedy.community(graphlist[[y]], weights = NULL)
  V(graphlist[[y]])$membership <- fc$membership
  clusts[y] <- length(fc)
  mods[y] <- modularity(graphlist[[y]], fc$membership)
}
plot(clusters)
plot(mods)
```

#Plots con clusters
Ideas para evaluar como se reparten "shared" entre clusters
1. Ver que porcentaje de las ASV de un cluster pertenecen a cada categoría
2. Ver que cluster contiene la mayoría de las shared y evaluar cual es el porcentaje.
```{r pclust, echo=TRUE, message=FALSE, warning=TRUE, include=TRUE}
sites <- c("Chañaral", "Flamenco", "Huasco", "Punta Choros", "Quintero", "Las Cruces")
par(mfrow = c(2,3))
for (x in 1:6) {
  set.seed(1000)
  plot(graphlist[[x]], edge.color = "grey", vertex.label = NA, 
       vertex.color = vertex_attr(graphlist[[x]], "membership"), 
       vertez.size = log(V(graphlist[[x]])$degree+2), 
       edge.width = 0.1/10000, layout = layout.fruchterman.reingold, 
       main = sites[x],
       sub = str_glue("N:{gorder(graphlist[[x]])}  E:{gsize(graphlist[[x]])}"))
}
par(new = FALSE)
```

#this
```{r}
t <- list()
for (y in 1:6) {
  t[[y]] <- as.data.frame(t(read.table(str_glue("t{y}.csv"), sep = ";", dec = ".", header = FALSE, row.names = 1))) %>% 
    dplyr::select(V1)
}
test <- t %>% unlist() %>% inner_join(by = "V1")
```
